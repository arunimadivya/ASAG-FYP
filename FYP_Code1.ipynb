{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67476e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bert_embedding import BertEmbedding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.isotonic import IsotonicRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6f2e0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5326f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function(Stopword removal and question demoting)\n",
    "def preprocessing(q, ans):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    q_tokens = word_tokenize(q)\n",
    "    ans_tokens = word_tokenize(ans)\n",
    "    demoted_tokens = [t for t in ans_tokens if t not in q_tokens]\n",
    "    filtered_sent = [w for w in demoted_tokens if not w in stop_words]\n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc20d7e",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f811b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics functions : rmse and pearson correlation\n",
    "def RMSE(actual, pred):\n",
    "    return sqrt(mean_squared_error(actual, pred))\n",
    "\n",
    "def Pearson(actual,pred):\n",
    "    mean_a = sum(actual) / len(actual)\n",
    "    mean_p = sum(pred) / len(pred)\n",
    "    cov = sum((a - mean_a) * (b - mean_p) for (a, b) in zip(actual, pred)) / len(actual)\n",
    "    p = float(cov / (np.std(actual) * np.std(pred)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02624637",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751e2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokens(sent):\n",
    "    if not list:\n",
    "        sent = word_tokenize(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fbcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(sent):\n",
    "    tokens = check_tokens(sent)\n",
    "    embedding = BertEmbedding().embedding(sentences=tokens)\n",
    "    word_arr = []\n",
    "    for i in range(len(embedding)):\n",
    "        word_arr.append(embedding[i][1][0])\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08649449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(sent):\n",
    "    tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'openai-gpt')\n",
    "    model = torch.hub.load('huggingface/pytorch-transformers', 'modelWithLMHead', 'openai-gpt')\n",
    "    \n",
    "    tokens = check_tokens(sent)\n",
    "    tokens_i = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_t = torch.tensor([tokens_i])\n",
    "    embedding = model(tokens_t)\n",
    "    \n",
    "    word_arr = []\n",
    "    for i in range(embedding[0].shape[1]):\n",
    "        word_arr.append(embedding[0][0][i].tolist())\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e3c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo(sent):\n",
    "    tokens = check_tokens(sent)\n",
    "    embedding = ElmoEmbedder().embed_sentence(tokens)\n",
    "    word_arr = []\n",
    "\n",
    "    for i in range(len(embedding[2])):\n",
    "        word_arr.append(embedding[0][i])\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e58a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(sent):\n",
    "    tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')\n",
    "    model = torch.hub.load('huggingface/pytorch-transformers', 'modelWithLMHead', 'gpt2')\n",
    "\n",
    "    tokens=check_tokens(sent)\n",
    "    tokens_i = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_t = torch.tensor([tokens_i])\n",
    "    embedding = model(tokens_t)\n",
    "\n",
    "    word_arr = []\n",
    "    for i in range(embedding[0].size()[1]):\n",
    "        word_arr.append(embedding[0][0][i].tolist())\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93830218",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce7dadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(xTrain,yTrain,xTest):\n",
    "    xTrain = check_nan(xTrain.to_numpy().reshape(-1,1))\n",
    "    yTrain = check_nan(yTrain.to_numpy().reshape(-1, 1))\n",
    "    xTest = check_nan(xTest.to_numpy().reshape(-1, 1))\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    return y_pred\n",
    "\n",
    "def isotonic_reg(xTrain,yTrain,xTest):\n",
    "    xTrain = xTrain.to_list()\n",
    "    yTrain = yTrain.to_list()\n",
    "    xTest = xTest.to_list()\n",
    "\n",
    "    model = IsotonicRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    return y_pred\n",
    "\n",
    "def ridge_reg(xTrain, yTrain, xTest):\n",
    "    xTrain = check_nan(xTrain.to_numpy().reshape(-1, 1))\n",
    "    yTrain = check_nan(yTrain.to_numpy().reshape(-1, 1))\n",
    "    xTest = check_nan(xTest.to_numpy().reshape(-1, 1))\n",
    "\n",
    "    model = Ridge()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73323a9",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Given the type of embedding to be used, the embeddings of model answers and student answers are generated. Then the cosine similarity score between the 2 are calculated and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/mohler_dataset_edited.csv')\n",
    "\n",
    "# Get  student answers from dataset\n",
    "student_answers = df['student_answer'].to_list()\n",
    "similarity_scores = {}\n",
    "model = str(input('Enter a model name(bert, gpt2, elmo, gpt) to get similarity scores for: '))\n",
    "\n",
    "#Calculate cosine similarity score for each answer\n",
    "for ans in student_answers:\n",
    "    q = df.loc[df['student_answer'] == ans, 'question'].iloc[0]\n",
    "    model_ans = df.loc[df['student_answer'] == ans, 'desired_answer'].iloc[0]\n",
    "\n",
    "    # Preprocess student answer\n",
    "    model_preproc = preprocessing(q, model_ans)\n",
    "    stu_preproc = preprocessing(q, ans)\n",
    "\n",
    "    # Calculate and save similarity score\n",
    "    if model==\"bert\":\n",
    "        model_arr = bert(model_preproc)\n",
    "        stu_arr = bert(stu_preproc)\n",
    "        similarity_scores[ans]=1-scipy.spatial.distance.cosine(sum(model_arr),sum(stu_arr))\n",
    "\n",
    "    elif model ==\"gpt2\":\n",
    "        model_arr = gpt2(model_preproc)\n",
    "        stu_arr = gpt2(stu_preproc)\n",
    "\n",
    "        e1 = [sum(i) for i in zip(*model_arr)]\n",
    "        e2 = [sum(i) for i in zip(*stu_arr)]\n",
    "        similarity_scores[ans] = 1-scipy.spatial.distance.cosine(e1,e2)\n",
    "\n",
    "    elif model==\"gpt\":\n",
    "        model_arr = gpt(model_preproc)\n",
    "        stu_arr = gpt(stu_preproc)\n",
    "        e1 = [sum(i) for i in zip(*model_arr)]\n",
    "        e2 = [sum(i) for i in zip(*stu_arr)]\n",
    "        similarity_scores[ans] = 1-scipy.spatial.distance.cosine(e1,e2)\n",
    "\n",
    "    elif model==\"elmo\":\n",
    "        model_arr = elmo(model_preproc)\n",
    "        stu_arr = elmo(stu_preproc)\n",
    "        similarity_scores[ans] = 1-scipy.spatial.distance.cosine(sum(model_arr), sum(stu_arr))\n",
    "    print('SCORE',similarity_scores)\n",
    "\n",
    "col_name=model+\"_similarity_score\"\n",
    "for a in student_answers:\n",
    "    df.loc[df['student_answer'] == a, col_name] = similarity_scores[a]\n",
    "\n",
    "# Apply normalization techniques\n",
    "columns = ['bert_similarity_score','elmo_similarity_score','gpt_similarity_score','gpt2_similarity_score']\n",
    "for c in columns:\n",
    "    df['normalized_'+c] = MinMaxScaler().fit_transform(np.array(df[c]).reshape(-1,1))\n",
    "    \n",
    "df.to_csv('dataset/answers_with_similarity_score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f8ef",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The similarity scores are used as data to train different regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "195938b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(l):\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def check_nan(arr):\n",
    "    idx_NaN = np.isnan(arr)\n",
    "    arr[idx_NaN] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "181f86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(mdl, df):\n",
    "    mask = np.random.rand(len(df)) < (70 / 100)\n",
    "    dataTrain = df[mask]\n",
    "    dataTest = df[~mask]\n",
    "\n",
    "    mdl_score = 'normalized_' + mdl.lower() + '_similarity_score'\n",
    "    xTrain = dataTrain[mdl_score]\n",
    "    xTest = dataTest[mdl_score]\n",
    "    yTrain = dataTrain['score_avg']\n",
    "    yTest = dataTest['score_avg'].to_list()\n",
    "\n",
    "    linear_yPred = [float(x) for x in linear_reg(xTrain,yTrain,xTest)]\n",
    "    ridge_yPred = [float(x) for x in ridge_reg(xTrain,yTrain,xTest)]\n",
    "    isotonic_yPred = list(np.nan_to_num(isotonic_reg(xTrain,yTrain,xTest), nan=0))\n",
    "\n",
    "    y = check_nan(np.asarray(yTest))\n",
    "    isotonic_pred = check_nan(np.asarray([round(i*2)/2  for i in isotonic_yPred]))\n",
    "    linear_pred = check_nan(np.asarray([round(i*2)/2  for i in linear_yPred]))\n",
    "    ridge_pred = check_nan(np.asarray([round(i*2)/2  for i in ridge_yPred]))\n",
    "\n",
    "    return RMSE(y,isotonic_pred), Pearson(y,isotonic_pred), RMSE(y,linear_pred), Pearson(y,linear_pred), RMSE(y,ridge_pred), Pearson(y,ridge_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c39ffe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "BERT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.06  \t Pearson Correlation : 0.319\n",
      "Linear Regression \t ==> \t RMSE : 1.081  \t Pearson Correlation : 0.266\n",
      "Ridge Regression \t ==> \t RMSE : 1.078  \t Pearson Correlation : 0.269\n",
      "ELMO\n",
      "Isotonic Regression \t ==> \t RMSE : 0.98  \t Pearson Correlation : 0.482\n",
      "Linear Regression \t ==> \t RMSE : 0.996  \t Pearson Correlation : 0.451\n",
      "Ridge Regression \t ==> \t RMSE : 0.997  \t Pearson Correlation : 0.449\n",
      "GPT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.083  \t Pearson Correlation : 0.25\n",
      "Linear Regression \t ==> \t RMSE : 1.088  \t Pearson Correlation : 0.227\n",
      "Ridge Regression \t ==> \t RMSE : 1.089  \t Pearson Correlation : 0.22\n",
      "GPT2\n",
      "Isotonic Regression \t ==> \t RMSE : 1.066  \t Pearson Correlation : 0.312\n",
      "Linear Regression \t ==> \t RMSE : 1.079  \t Pearson Correlation : 0.273\n",
      "Ridge Regression \t ==> \t RMSE : 1.08  \t Pearson Correlation : 0.268\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/answers_with_similarity_score.csv')\n",
    "print(\"Evaluation Results:\")\n",
    "models=[\"bert\",\"elmo\",\"gpt\",\"gpt2\"]\n",
    "for m in models:\n",
    "    print(m.upper())\n",
    "    rmse_iso,rmse_lin,rmse_rid = [],[],[]\n",
    "    pc_iso, pc_lin, pc_rid = [], [],[]\n",
    "\n",
    "    for i in range(0, 1000):\n",
    "        iso_rmse_score, iso_pc_score, lin_rmse_score, lin_pc_score, rid_rmse_score, rid_pc_score = evaluate_models(m,df)\n",
    "        rmse_iso.append(iso_rmse_score)\n",
    "        pc_iso.append(iso_pc_score)\n",
    "\n",
    "        rmse_lin.append(lin_rmse_score)\n",
    "        pc_lin.append(lin_pc_score)\n",
    "        \n",
    "        rmse_rid.append(rid_rmse_score)\n",
    "        pc_rid.append(rid_pc_score)\n",
    "\n",
    "\n",
    "    print(\"Isotonic Regression \\t ==> \\t RMSE :\",round(avg(rmse_iso), 3),\" \\t Pearson Correlation :\", round(avg(pc_iso), 3))\n",
    "    print(\"Linear Regression \\t ==> \\t RMSE :\",round(avg(rmse_lin), 3),\" \\t Pearson Correlation :\", round(avg(pc_lin), 3))\n",
    "    print(\"Ridge Regression \\t ==> \\t RMSE :\",round(avg(rmse_rid), 3),\" \\t Pearson Correlation :\", round(avg(pc_rid), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2597b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
